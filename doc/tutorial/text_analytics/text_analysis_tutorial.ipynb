{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is my attempt to run the exercises in the text analysis tutorial on the sklearn website. The tutorial begins [here](http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html), but the part I am doing in this notebook is the exercises which require some cloning and copying of files. So I am starting at **Exercises** which is down toward the end of the page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% cp -r skeletons workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/michaelreinhard/scikit-learn/doc/tutorial/text_analytics/workspace\n"
     ]
    }
   ],
   "source": [
    "% cd workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so maybe I am not doing this right. The instructions say that the skeleton file has all the necessary import statements and stuff. Maybe that means that I am supposed to open the notebooks in the workspace? \n",
    "\n",
    "Ok, there are no ipython notebooks in the workspace but there are scrips that I am probably supposed to run. So I am just going to run the code using a magic command as if I were doing it from the command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/michaelreinhard/scikit-learn/doc/tutorial/text_analytics\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mdata\u001b[m\u001b[m/                              working_with_text_data.rst\r\n",
      "\u001b[34mskeletons\u001b[m\u001b[m/                         working_with_text_data_fixture.py\r\n",
      "\u001b[34msolutions\u001b[m\u001b[m/                         \u001b[34mworkspace\u001b[m\u001b[m/\r\n",
      "text_analysis_tutorial.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named model_selection",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/Users/michaelreinhard/scikit-learn/doc/tutorial/text_analytics/workspace/exercise_01_language_train_model.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named model_selection"
     ]
    }
   ],
   "source": [
    "%run workspace/exercise_01_language_train_model.py data/languages/paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/Users/michaelreinhard/scikit-learn/doc/tutorial/text_analytics/workspace/exercise_01_language_train_model.py\u001b[0m(18)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     17 \u001b[0;31m\u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 18 \u001b[0;31m\u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     19 \u001b[0;31m\u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> sklearn.model_selection.train_test_split(*arrays, **options)[source]\n",
      "*** NameError: name 'sklearn' is not defined\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\"\"Build a language detector model\r\n",
      "\r\n",
      "The goal of this exercise is to train a linear classifier on text features\r\n",
      "that represent sequences of up to 3 consecutive characters so as to be\r\n",
      "recognize natural languages by using the frequencies of short character\r\n",
      "sequences as 'fingerprints'.\r\n",
      "\r\n",
      "\"\"\"\r\n",
      "# Author: Olivier Grisel <olivier.grisel@ensta.org>\r\n",
      "# License: Simplified BSD\r\n",
      "\r\n",
      "import sys\r\n",
      "\r\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
      "from sklearn.linear_model import Perceptron\r\n",
      "from sklearn.pipeline import Pipeline\r\n",
      "from sklearn.datasets import load_files\r\n",
      "from sklearn.model_selection import train_test_split\r\n",
      "from sklearn import metrics\r\n",
      "\r\n",
      "\r\n",
      "# The training data folder must be passed as first argument\r\n",
      "languages_data_folder = sys.argv[1]\r\n",
      "dataset = load_files(languages_data_folder)\r\n",
      "\r\n",
      "# Split the dataset in training and test set:\r\n",
      "docs_train, docs_test, y_train, y_test = train_test_split(\r\n",
      "    dataset.data, dataset.target, test_size=0.5)\r\n",
      "\r\n",
      "\r\n",
      "# TASK: Build a an vectorizer that splits strings into sequence of 1 to 3\r\n",
      "# characters instead of word tokens\r\n",
      "\r\n",
      "# TASK: Build a vectorizer / classifier pipeline using the previous analyzer\r\n",
      "# the pipeline instance should stored in a variable named clf\r\n",
      "\r\n",
      "# TASK: Fit the pipeline on the training set\r\n",
      "\r\n",
      "# TASK: Predict the outcome on the testing set in a variable named y_predicted\r\n",
      "\r\n",
      "# Print the classification report\r\n",
      "print(metrics.classification_report(y_test, y_predicted,\r\n",
      "                                    target_names=dataset.target_names))\r\n",
      "\r\n",
      "# Plot the confusion matrix\r\n",
      "cm = metrics.confusion_matrix(y_test, y_predicted)\r\n",
      "print(cm)\r\n",
      "\r\n",
      "#import pylab as pl\r\n",
      "#pl.matshow(cm, cmap=pl.cm.jet)\r\n",
      "#pl.show()\r\n",
      "\r\n",
      "# Predict the result on some short new sentences:\r\n",
      "sentences = [\r\n",
      "    u'This is a language detection test.',\r\n",
      "    u'Ceci est un test de d\\xe9tection de la langue.',\r\n",
      "    u'Dies ist ein Test, um die Sprache zu erkennen.',\r\n",
      "]\r\n",
      "predicted = clf.predict(sentences)\r\n",
      "\r\n",
      "for s, p in zip(sentences, predicted):\r\n",
      "    print(u'The language of \"%s\" is \"%s\"' % (s, dataset.target_names[p]))\r\n"
     ]
    }
   ],
   "source": [
    "%cat workspace/exercise_01_language_train_model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build a language detector model**\n",
    "\n",
    "The goal of this exercise is to train a linear classifier on text features\n",
    "that represent sequences of up to 3 consecutive characters so as to be\n",
    "recognize natural languages by using the frequencies of short character\n",
    "sequences as 'fingerprints'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
